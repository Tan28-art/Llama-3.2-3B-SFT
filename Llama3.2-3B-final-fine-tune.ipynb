{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a2572324-d137-4d8b-8732-77aaae838124",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Cell 1: Install Libraries\n",
    "!pip install -q torch transformers datasets accelerate peft bitsandbytes trl sentencepiece\n",
    "!pip install -q tensorboard\n",
    "#!pip install -q flash-attn --no-build-isolation # Install flash-attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "14d0d516-bfe9-4a66-af3e-fb0716d33093",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1.5: Install Evaluation Libraries\n",
    "!pip install -q evaluate bert_score rouge_score nltk \n",
    "# Download nltk punkt data if needed (often used by rouge_score)\n",
    "import nltk\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "except (LookupError, OSError):\n",
    "    print(\"Downloading nltk punkt tokenizer...\")\n",
    "    nltk.download('punkt', quiet=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4544c853-e84a-44e2-8d74-7da7028bddba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imports completed.\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Imports\n",
    "import json\n",
    "import os\n",
    "import random\n",
    "import torch\n",
    "import math # For ceiling function\n",
    "import traceback # For error details\n",
    "import re # For text processing\n",
    "import string # For text processing\n",
    "from datasets import load_dataset, concatenate_datasets, DatasetDict, Dataset\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments, # Base class, SFTConfig inherits from this\n",
    "    pipeline,\n",
    "    logging,\n",
    ")\n",
    "from peft import LoraConfig, PeftModel, prepare_model_for_kbit_training, get_peft_model\n",
    "from trl import SFTTrainer, SFTConfig, DataCollatorForCompletionOnlyLM # Added SFTConfig and Collator\n",
    "from huggingface_hub import login\n",
    "\n",
    "# Set environment variable for tokenizer parallelism (optional, can help with warnings)\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "print(\"Imports completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9900ceb4-d79b-49c9-a5ce-0c4e23594998",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration set.\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Configuration\n",
    "model_name = \"meta-llama/Llama-3.2-3B\"\n",
    "adapter_output_dir = \"./llama3.2-3b-sft-adapters-gen-prompt\" # New dir for this run\n",
    "#adapter_output_dir = \".\" # New dir for this run\n",
    "logs_output_dir = \"./llama3.2-3b-sft-logs-gen-prompt\"      # New dir for logs\n",
    "\n",
    "# SFT / Training Params\n",
    "max_samples_per_dataset = 10000 # Limit samples per source\n",
    "max_seq_length = 512           # Sequence length\n",
    "train_batch_size = 32          # Increase for 80GB GPU\n",
    "gradient_accumulation_steps = 1  # Effective batch size = 16*2 = 32\n",
    "eval_batch_size = 32           # Increase eval batch size too\n",
    "learning_rate = 2e-4           # Starting learning rate\n",
    "num_train_epochs = 1           # INCREASED: Train for longer\n",
    "lr_scheduler_type = \"cosine\"   # Learning rate scheduler\n",
    "warmup_ratio = 0.03            # Warmup steps proportion\n",
    "weight_decay = 0.001           # Weight decay for regularization\n",
    "optim_name = \"paged_adamw_32bit\" # Optimizer\n",
    "\n",
    "# LoRA Params\n",
    "lora_r = 64\n",
    "lora_alpha = 16\n",
    "lora_dropout = 0.1\n",
    "lora_target_modules = [ # Common targets for Llama-like models\n",
    "    \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "    \"gate_proj\", \"up_proj\", \"down_proj\",\n",
    "]\n",
    "\n",
    "# QLoRA Params\n",
    "quant_load_in_4bit = True\n",
    "quant_bnb_4bit_quant_type = \"nf4\"\n",
    "quant_bnb_4bit_use_double_quant = True\n",
    "\n",
    "# Other Params\n",
    "seed = 42 # For reproducibility\n",
    "\n",
    "print(\"Configuration set.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "529525bd-3e1a-4ec0-ad01-8b53cafb56f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available. GPU: NVIDIA A100-SXM4-80GB MIG 2g.20gb\n",
      "Number of GPUs available: 1\n",
      "Tensor on GPU: tensor([[-0.9638,  0.4505,  1.5830],\n",
      "        [ 0.7932,  0.1194, -0.1534]], device='cuda:0')\n",
      "CUDA version: 12.4\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Check if CUDA is available\n",
    "if torch.cuda.is_available():\n",
    "    # Get the name of the current GPU\n",
    "    gpu_name = torch.cuda.get_device_name(0)\n",
    "    print(f\"CUDA is available. GPU: {gpu_name}\")\n",
    "\n",
    "    # Get the number of available GPUs\n",
    "    num_gpus = torch.cuda.device_count()\n",
    "    print(f\"Number of GPUs available: {num_gpus}\")\n",
    "\n",
    "    # Allocate a tensor on the GPU\n",
    "    device = torch.device(\"cuda\")\n",
    "    a = torch.randn(2, 3).to(device)\n",
    "    print(f\"Tensor on GPU: {a}\")\n",
    "\n",
    "    # Get CUDA version\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")\n",
    "else:\n",
    "    print(\"CUDA is not available. PyTorch is running on CPU.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "756092ba-e3f1-40f7-98e8-53d71e092a7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading datasets (max 10000 samples per dataset)...\n",
      "Loading alpaca...\n",
      "  Subsetting alpaca from 52002 to 10000 samples.\n",
      "Loading dolly...\n",
      "  Subsetting dolly from 15011 to 10000 samples.\n",
      "Loading hh_rlhf...\n",
      "  Subsetting hh_rlhf from 160800 to 10000 samples.\n",
      "Loading gsm8k...\n",
      "  Loaded 7473 samples (within limit).\n",
      "Loading metamathqa...\n",
      "  Subsetting metamathqa from 395000 to 10000 samples.\n",
      "Loading hotpot_qa...\n",
      "  Subsetting hotpot_qa from 90447 to 10000 samples.\n",
      "Loading mmlu...\n",
      "  Subsetting mmlu from 99842 to 10000 samples.\n",
      "Loading commonsense_qa...\n",
      "  Loaded 9741 samples (within limit).\n",
      "Loading trivia_qa...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e1db383a8c94f87b20e4a2a56ddc038",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/26 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Subsetting trivia_qa from 138384 to 10000 samples.\n",
      "Loading truthful_qa...\n",
      "  Loaded 817 samples (within limit).\n",
      "\n",
      "Finished loading datasets. Total datasets loaded: 10\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Load ALL Datasets (Limit each to 10k max)\n",
    "import random # Ensure imported\n",
    "\n",
    "print(f\"Loading datasets (max {max_samples_per_dataset} samples per dataset)...\")\n",
    "loaded_datasets = {} # Dictionary to hold the loaded and subsetted datasets\n",
    "\n",
    "dataset_sources = {\n",
    "    \"alpaca\": (\"tatsu-lab/alpaca\", None, \"train\"),\n",
    "    \"dolly\": (\"databricks/databricks-dolly-15k\", None, \"train\"),\n",
    "    \"hh_rlhf\": (\"Anthropic/hh-rlhf\", None, \"train\"),\n",
    "    \"gsm8k\": (\"gsm8k\", \"main\", \"train\"),\n",
    "    \"metamathqa\": (\"meta-math/MetaMathQA\", None, \"train\"),\n",
    "    \"hotpot_qa\": (\"hotpot_qa\", \"fullwiki\", \"train\"),\n",
    "    \"mmlu\": (\"cais/mmlu\", \"all\", \"auxiliary_train\"),\n",
    "    \"commonsense_qa\": (\"commonsense_qa\", None, \"train\"),\n",
    "    \"trivia_qa\": (\"trivia_qa\", \"rc.nocontext\", \"train\"),\n",
    "    \"truthful_qa\": (\"domenicrosati/TruthfulQA\", None, \"train\")\n",
    "}\n",
    "\n",
    "try:\n",
    "    for key, (path, name, split) in dataset_sources.items():\n",
    "        print(f\"Loading {key}...\")\n",
    "        ds = load_dataset(path, name=name, split=split, trust_remote_code=True)\n",
    "\n",
    "        # Shuffle before selecting for randomness\n",
    "        ds_shuffled = ds.shuffle(seed=seed) # Use configured seed\n",
    "\n",
    "        # Select subset if larger than max_samples_per_dataset\n",
    "        if len(ds_shuffled) > max_samples_per_dataset:\n",
    "            print(f\"  Subsetting {key} from {len(ds_shuffled)} to {max_samples_per_dataset} samples.\")\n",
    "            ds_subset = ds_shuffled.select(range(max_samples_per_dataset))\n",
    "        else:\n",
    "            ds_subset = ds_shuffled # Use the whole shuffled dataset if smaller\n",
    "            print(f\"  Loaded {len(ds_subset)} samples (within limit).\")\n",
    "\n",
    "        loaded_datasets[key] = ds_subset # Store the subsetted dataset\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error loading or subsetting datasets: {e}\")\n",
    "    print(traceback.format_exc())\n",
    "    raise e\n",
    "\n",
    "print(f\"\\nFinished loading datasets. Total datasets loaded: {len(loaded_datasets)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f45f7147-025a-40a5-96eb-e4e7bbba9b5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32094d80755d4fa88a8b0ac7904f84c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0c4e6f41-7297-472f-b54a-dfb2923a1e2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tokenizer...\n",
      "Setting pad_token = eos_token\n",
      "Tokenizer loaded (vocab size: 128000). Pad token ID: 128001\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: Load Tokenizer\n",
    "from transformers import AutoTokenizer # Ensure imported\n",
    "\n",
    "print(\"Loading tokenizer...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True, use_fast=False, legacy=False)\n",
    "tokenizer.padding_side = \"right\" # Padding side for training\n",
    "if tokenizer.pad_token is None:\n",
    "    print(\"Setting pad_token = eos_token\")\n",
    "    tokenizer.pad_token = tokenizer.eos_token # Needed for training if model lacks pad token\n",
    "print(f\"Tokenizer loaded (vocab size: {tokenizer.vocab_size}). Pad token ID: {tokenizer.pad_token_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e743e091-1da5-47f5-b008-8b1858f0fb1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All formatting functions updated for robustness (including TriviaQA fix).\n"
     ]
    }
   ],
   "source": [
    "# Cell 6: Formatting Functions (More Robust) - REVISED format_trivia_qa\n",
    "\n",
    "import json\n",
    "import traceback\n",
    "import re\n",
    "\n",
    "# --- Base Prompt Creation Function ---\n",
    "# (Keep your existing create_prompt function as is)\n",
    "def create_prompt(instruction, input_text=None, output=\"\", include_eos=True):\n",
    "    instruction_str = str(instruction).strip() if instruction is not None else \"[No Instruction]\"\n",
    "    input_text_str = str(input_text).strip() if input_text is not None else \"\"\n",
    "    output_str = str(output) if output is not None else \"\"\n",
    "    if input_text_str: prompt = f\"\"\"... Instruction:\\n{instruction_str}\\n\\n### Input:\\n{input_text_str}\\n\\n### Response:\\n{output_str}\"\"\" # Simplified for brevity\n",
    "    else: prompt = f\"\"\"... Instruction:\\n{instruction_str}\\n\\n### Response:\\n{output_str}\"\"\" # Simplified\n",
    "    if include_eos: prompt += tokenizer.eos_token if 'tokenizer' in globals() else \"<|end_of_text|>\"\n",
    "    return prompt\n",
    "\n",
    "# --- Formatting Functions with Added Checks ---\n",
    "\n",
    "def format_alpaca(sample):\n",
    "    instruction = sample.get('instruction', '[No Instruction]')\n",
    "    input_data = sample.get('input')\n",
    "    output = sample.get('output', '[No Output]')\n",
    "    return {'text': create_prompt(instruction, input_data, output)}\n",
    "\n",
    "def format_dolly(sample):\n",
    "    instruction = sample.get('instruction', '[No Instruction]')\n",
    "    context = sample.get('context')\n",
    "    response = sample.get('response', '[No Response]')\n",
    "    category = sample.get('category')\n",
    "    if category: instruction = f\"Category: {category}. Task: {instruction}\" # Add category if present\n",
    "    return {'text': create_prompt(instruction, context, response)}\n",
    "\n",
    "def format_hh_rlhf(sample):\n",
    "    prompt_full = sample.get('chosen', ''); instruction = prompt_full; input_text = None; output = \"[Parse Error]\"\n",
    "    last_human_idx = prompt_full.rfind(\"\\n\\nHuman:\"); last_assistant_idx = prompt_full.rfind(\"\\n\\nAssistant:\")\n",
    "    if last_human_idx != -1 and last_assistant_idx != -1 and last_assistant_idx > last_human_idx:\n",
    "        instruction = prompt_full[last_human_idx + len(\"\\n\\nHuman:\"):last_assistant_idx].strip()\n",
    "        output = prompt_full[last_assistant_idx + len(\"\\n\\nAssistant:\"):].strip()\n",
    "    return {'text': create_prompt(instruction, input_text, output)}\n",
    "\n",
    "def format_gsm8k(sample):\n",
    "    instruction = sample.get('question', '[No Question]')\n",
    "    answer_solution = sample.get('answer', '[No Answer]') # Should contain CoT\n",
    "    return {'text': create_prompt(instruction, None, answer_solution)}\n",
    "\n",
    "def format_metamathqa(sample):\n",
    "    instruction = sample.get('query', '[No Query]')\n",
    "    response = sample.get('response', '[No Response]') # Should contain CoT\n",
    "    return {'text': create_prompt(instruction, None, response)}\n",
    "\n",
    "def format_hotpot_qa(sample):\n",
    "    instruction = sample.get('question', '[No Question]')\n",
    "    answer = sample.get('answer', '[No Answer]')\n",
    "    context = sample.get('context') # This field was removed before concat, but is present in raw sample\n",
    "    context_str = \"[No Context Provided]\"\n",
    "    # Check if context is the expected dictionary structure\n",
    "    if isinstance(context, dict):\n",
    "         titles = context.get('title', []); sentences = context.get('sentences', [])\n",
    "         if isinstance(titles, list) and isinstance(sentences, list) and len(titles) == len(sentences):\n",
    "              context_parts = []\n",
    "              for title, sentence_list in zip(titles, sentences):\n",
    "                   safe_sentences = [str(s) for s in sentence_list] if isinstance(sentence_list, list) else [str(sentence_list)]\n",
    "                   context_parts.append(f\"Document Title: {str(title)}\\nContent: {' '.join(safe_sentences)}\")\n",
    "              context_str = \"\\n\\n\".join(context_parts)\n",
    "         else: context_str = str(context) # Fallback if structure invalid\n",
    "    elif context is not None: context_str = str(context) # Handle non-dict context\n",
    "\n",
    "    return {'text': create_prompt(instruction, context_str, answer)}\n",
    "\n",
    "def format_mmlu(sample):\n",
    "    instruction = sample.get('question', '[No Question]')\n",
    "    # IMPORTANT: Access original 'choices' list here before it's removed in Cell 7 prep\n",
    "    choices = sample.get('choices', [])\n",
    "    # Access standardized string 'answer_standardized' prepared in Cell 7 prep\n",
    "    answer_str = sample.get('answer_standardized', '[No Standardized Answer]') # Use the standardized string\n",
    "    subject = sample.get('subject', 'Unknown Subject')\n",
    "\n",
    "    input_text = f\"Subject: {subject}\\nChoices:\\n\"\n",
    "    if isinstance(choices, list) and len(choices) > 0:\n",
    "         options = \"\\n\".join([f\"{chr(65+i)}) {choice}\" for i, choice in enumerate(choices)])\n",
    "         input_text += options\n",
    "    else: input_text += \"[Choices not available]\"\n",
    "\n",
    "    # The 'output' is the pre-formatted answer string from standardization\n",
    "    output = answer_str\n",
    "    return {'text': create_prompt(instruction, input_text, output)}\n",
    "\n",
    "def format_commonsense_qa(sample):\n",
    "    instruction = sample.get('question', '[No Question]')\n",
    "    # IMPORTANT: Access original 'choices' dict here before it's removed\n",
    "    choices_dict = sample.get('choices', {})\n",
    "    answer_key = sample.get('answerKey') # This is the label 'A', 'B' etc.\n",
    "\n",
    "    input_text = \"Choices:\\n\"\n",
    "    choices_text = choices_dict.get('text', [])\n",
    "    choices_labels = choices_dict.get('label', [])\n",
    "    if isinstance(choices_text, list) and isinstance(choices_labels, list):\n",
    "         options = \"\\n\".join([f\"{label}) {choice}\" for label, choice in zip(choices_labels, choices_text)])\n",
    "         input_text += options\n",
    "    else: input_text += \"[Choices unavailable]\"\n",
    "\n",
    "    # Reconstruct the target output string using the answerKey\n",
    "    output = \"[Invalid Answer Key]\"\n",
    "    if answer_key and isinstance(choices_text, list) and isinstance(choices_labels, list):\n",
    "        try:\n",
    "            answer_idx = choices_labels.index(answer_key)\n",
    "            output = f\"{answer_key}) {choices_text[answer_idx]}\"\n",
    "        except (ValueError, IndexError): output = \"[Answer Key not found]\"\n",
    "\n",
    "    return {'text': create_prompt(instruction, input_text, output)}\n",
    "\n",
    "# --- CORRECTED TriviaQA Formatter ---\n",
    "def format_trivia_qa(sample):\n",
    "    instruction = sample.get('question', '[No Question]')\n",
    "    # Access the STANDARDIZED string 'answer_standardized' prepared in Cell 7 prep\n",
    "    # NOT the original 'answer' dictionary.\n",
    "    answer_value = sample.get('answer_standardized', '[No Standardized Answer]')\n",
    "    return {'text': create_prompt(instruction, None, answer_value)}\n",
    "# --- END CORRECTION ---\n",
    "\n",
    "def format_truthful_qa(sample):\n",
    "    instruction = sample.get('Question', '[No Question]') # Note 'Question' capitalization\n",
    "    best_answer = sample.get('Best Answer', '[No Answer]')\n",
    "    return {'text': create_prompt(instruction, None, best_answer)}\n",
    "\n",
    "# --- Formatting Function for Evaluation Dataset (dev_data.json) ---\n",
    "# (Keep previous robust version)\n",
    "def format_dev_subset(sample):\n",
    "    try:\n",
    "        instruction = sample.get('question', '[No Question]'); answer_data = sample.get('answer'); context_data = sample.get('context')\n",
    "        input_text = context_data if context_data else None; output = str(answer_data) if answer_data is not None else \"\"\n",
    "        # Add basic context marker if input exists\n",
    "        if input_text: input_text = f\"Context:\\n{input_text}\"\n",
    "        formatted_prompt = create_prompt(instruction, input_text, output)\n",
    "        return {'text': formatted_prompt}\n",
    "    except Exception as e: print(f\"Eval Format Error: {e}\"); return {'text': f\"[Error]\"}\n",
    "\n",
    "\n",
    "print(\"All formatting functions updated for robustness (including TriviaQA fix).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c94ad2d4-1408-46ca-b933-2eeee77d7789",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Preparing Training & Evaluation Data ---\n",
      "Stage 1: Processing and Formatting each training dataset...\n",
      "  Processing and Formatting alpaca (10000 samples)...\n",
      "    Applying final formatter for alpaca...\n",
      "    ... Final formatting for alpaca complete.\n",
      "  Processing and Formatting dolly (10000 samples)...\n",
      "    Applying final formatter for dolly...\n",
      "    ... Final formatting for dolly complete.\n",
      "  Processing and Formatting hh_rlhf (10000 samples)...\n",
      "    Applying final formatter for hh_rlhf...\n",
      "    ... Final formatting for hh_rlhf complete.\n",
      "  Processing and Formatting gsm8k (7473 samples)...\n",
      "    Applying final formatter for gsm8k...\n",
      "    ... Final formatting for gsm8k complete.\n",
      "  Processing and Formatting metamathqa (10000 samples)...\n",
      "    Applying final formatter for metamathqa...\n",
      "    ... Final formatting for metamathqa complete.\n",
      "  Processing and Formatting hotpot_qa (10000 samples)...\n",
      "    Applying final formatter for hotpot_qa...\n",
      "    ... Final formatting for hotpot_qa complete.\n",
      "  Processing and Formatting mmlu (10000 samples)...\n",
      "    Standardizing MMLU 'answer' -> 'answer_standardized'...\n",
      "    ... MMLU standardization complete. New columns: ['question', 'subject', 'choices', 'answer', 'answer_standardized']\n",
      "    Applying final formatter for mmlu...\n",
      "    ... Final formatting for mmlu complete.\n",
      "  Processing and Formatting commonsense_qa (9741 samples)...\n",
      "    Applying final formatter for commonsense_qa...\n",
      "    ... Final formatting for commonsense_qa complete.\n",
      "  Processing and Formatting trivia_qa (10000 samples)...\n",
      "    Standardizing TriviaQA 'answer' -> 'answer_standardized'...\n",
      "    ... TriviaQA standardization complete. New columns: ['question', 'question_id', 'question_source', 'entity_pages', 'search_results', 'answer', 'answer_standardized']\n",
      "    Applying final formatter for trivia_qa...\n",
      "    ... Final formatting for trivia_qa complete.\n",
      "  Processing and Formatting truthful_qa (817 samples)...\n",
      "    Applying final formatter for truthful_qa...\n",
      "    ... Final formatting for truthful_qa complete.\n",
      "Concatenating all formatted training datasets...\n",
      "Created final formatted training dataset (`train_dataset`): 88031 samples\n",
      "Final Training Dataset Schema: {'text': Value(dtype='string', id=None)}\n",
      "\n",
      "Loading and preparing evaluation dataset from dev_data.json...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aaf8de7ddffd46d4b08f173d460a9915",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Formatting eval_dataset:   0%|          | 0/800 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12dc387f1f7341a3beb99d8f6a833e10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/800 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Formatted `eval_dataset` for Trainer size: 800\n",
      "\n",
      "--- Data Preparation Complete ---\n"
     ]
    }
   ],
   "source": [
    "# Cell 7: Process & Combine Subsets + Prep Eval Data (Corrected Structure)\n",
    "import traceback\n",
    "import json\n",
    "import os\n",
    "from datasets import concatenate_datasets, Dataset, ClassLabel, Value, Sequence, Features\n",
    "\n",
    "print(\"--- Preparing Training & Evaluation Data ---\")\n",
    "\n",
    "# --- Ensure loaded_datasets exists from Cell 4 ---\n",
    "if 'loaded_datasets' not in locals() or not loaded_datasets:\n",
    "    raise ValueError(\"`loaded_datasets` dictionary not found or empty. Please run Cell 4 first.\")\n",
    "\n",
    "# --- Define Formatter Map ---\n",
    "formatter_map = {\n",
    "    'alpaca': format_alpaca, 'dolly': format_dolly, 'hh_rlhf': format_hh_rlhf,\n",
    "    'gsm8k': format_gsm8k, 'metamathqa': format_metamathqa, 'hotpot_qa': format_hotpot_qa,\n",
    "    'mmlu': format_mmlu, 'commonsense_qa': format_commonsense_qa,\n",
    "    'trivia_qa': format_trivia_qa, 'truthful_qa': format_truthful_qa\n",
    "}\n",
    "\n",
    "# --- Process and Format Each Training Dataset Subset ---\n",
    "print(\"Stage 1: Processing and Formatting each training dataset...\")\n",
    "all_formatted_datasets = []\n",
    "num_proc = os.cpu_count()\n",
    "\n",
    "for key, ds in loaded_datasets.items():\n",
    "    print(f\"  Processing and Formatting {key} ({len(ds)} samples)...\")\n",
    "    ds_processed = ds # Start with the subset\n",
    "    current_features = ds_processed.features\n",
    "\n",
    "    try:\n",
    "        # 1. Standardize specific columns BEFORE formatting\n",
    "        if key == 'mmlu' and 'answer' in current_features and isinstance(current_features.get('answer'), ClassLabel):\n",
    "            print(f\"    Standardizing MMLU 'answer' -> 'answer_standardized'...\")\n",
    "            def mmlu_answer_to_string(example):\n",
    "                 choices = example.get('choices', []) # Need original choices\n",
    "                 answer_idx = example.get('answer', -1) # Original answer is int index\n",
    "                 if isinstance(choices, list) and 0 <= answer_idx < len(choices):\n",
    "                      try:\n",
    "                          # *** CORRECT: Return the new column name ***\n",
    "                          return {\"answer_standardized\": f\"{chr(65+answer_idx)}) {choices[answer_idx]}\"}\n",
    "                      except Exception as e_fmt:\n",
    "                          print(f\"DEBUG: MMLU format error - idx={answer_idx}, choices={choices}, error={e_fmt}\")\n",
    "                          return {\"answer_standardized\": \"[MMLU Fmt Err]\"}\n",
    "                 else:\n",
    "                     print(f\"DEBUG: MMLU index error - idx={answer_idx}, choices len={len(choices) if isinstance(choices, list) else 'Not List'}\")\n",
    "                     return {\"answer_standardized\": \"[MMLU Idx Err]\"}\n",
    "\n",
    "            # *** ADDED: Explicitly define the new feature type ***\n",
    "            new_features = ds_processed.features.copy()\n",
    "            new_features['answer_standardized'] = Value('string')\n",
    "\n",
    "            ds_processed = ds_processed.map(\n",
    "                mmlu_answer_to_string,\n",
    "                num_proc=num_proc,\n",
    "                desc=f\"Std MMLU ans\",\n",
    "                features=new_features # Pass the updated features schema\n",
    "            )\n",
    "            # Keep original answer/choices for the formatter, will be removed later\n",
    "            print(f\"    ... MMLU standardization complete. New columns: {list(ds_processed.features.keys())}\")\n",
    "\n",
    "\n",
    "        elif key == 'trivia_qa' and 'answer' in current_features and isinstance(current_features.get('answer'), dict):\n",
    "             print(f\"    Standardizing TriviaQA 'answer' -> 'answer_standardized'...\")\n",
    "             def triviaqa_answer_to_string(example):\n",
    "                 answer_dict = example.get('answer', {});\n",
    "                 # *** CORRECT: Return the new column name ***\n",
    "                 return {\"answer_standardized\": str(answer_dict.get('value', ''))}\n",
    "\n",
    "             # *** ADDED: Explicitly define the new feature type ***\n",
    "             new_features = ds_processed.features.copy()\n",
    "             new_features['answer_standardized'] = Value('string')\n",
    "\n",
    "             ds_processed = ds_processed.map(\n",
    "                 triviaqa_answer_to_string,\n",
    "                 num_proc=num_proc,\n",
    "                 desc=f\"Std TQA ans\",\n",
    "                 features=new_features # Pass the updated features schema\n",
    "             )\n",
    "             # Keep original answer dict for the formatter, will be removed later\n",
    "             print(f\"    ... TriviaQA standardization complete. New columns: {list(ds_processed.features.keys())}\")\n",
    "\n",
    "\n",
    "        # 2. Apply the specific formatter for this source key\n",
    "        if key in formatter_map:\n",
    "            formatter = formatter_map[key]\n",
    "            # Apply formatter, which returns {'text': ...}\n",
    "            # The formatter function needs access to original cols (like choices, context, standardized answer)\n",
    "            # It should read from ds_processed\n",
    "            print(f\"    Applying final formatter for {key}...\")\n",
    "            # Define columns to remove *before* mapping, ensuring 'text' isn't removed if it somehow exists\n",
    "            cols_to_remove = [col for col in ds_processed.column_names if col != 'text']\n",
    "\n",
    "            formatted_ds = ds_processed.map(\n",
    "                formatter,\n",
    "                # Remove all original columns AND intermediate standardized columns\n",
    "                remove_columns=cols_to_remove,\n",
    "                num_proc=num_proc,\n",
    "                desc=f\"Final Formatting {key}\"\n",
    "            )\n",
    "            # Verify schema after formatting\n",
    "            if list(formatted_ds.features.keys()) != ['text']:\n",
    "                 print(f\"Error: Formatter for {key} did not result in only 'text' column. Columns: {formatted_ds.column_names}. Skipping.\")\n",
    "                 # If error occurs here, print a sample of the problematic dataset before skipping\n",
    "                 try:\n",
    "                     print(\"Problematic sample before skipping:\", formatted_ds[0])\n",
    "                 except:\n",
    "                     print(\"Could not retrieve problematic sample.\")\n",
    "                 continue\n",
    "            all_formatted_datasets.append(formatted_ds)\n",
    "            print(f\"    ... Final formatting for {key} complete.\")\n",
    "        else:\n",
    "            print(f\"    Skipping {key}: No formatter defined in map.\")\n",
    "\n",
    "    except Exception as e_prep:\n",
    "         print(f\"Error processing dataset '{key}': {e_prep}\")\n",
    "         print(traceback.format_exc())\n",
    "         raise e_prep # Stop if preprocessing fails\n",
    "\n",
    "# Concatenate the final formatted datasets (each only has 'text')\n",
    "print(\"Concatenating all formatted training datasets...\")\n",
    "if not all_formatted_datasets: raise ValueError(\"No datasets were successfully formatted.\")\n",
    "# Ensure all datasets have the same simple 'text' feature before concatenating\n",
    "expected_features = Features({'text': Value(dtype='string', id=None)})\n",
    "compatible_datasets = []\n",
    "for ds in all_formatted_datasets:\n",
    "    if ds.features == expected_features:\n",
    "        compatible_datasets.append(ds)\n",
    "    else:\n",
    "        print(f\"Warning: Dataset skipped during concat due to incompatible features: {ds.features}. Expected: {expected_features}\")\n",
    "\n",
    "if not compatible_datasets: raise ValueError(\"No datasets with the correct final 'text' feature found for concatenation.\")\n",
    "train_dataset = concatenate_datasets(compatible_datasets) # Use compatible list\n",
    "train_dataset = train_dataset.shuffle(seed=seed) # Use seed from Cell 3\n",
    "print(f\"Created final formatted training dataset (`train_dataset`): {len(train_dataset)} samples\")\n",
    "print(f\"Final Training Dataset Schema: {train_dataset.features}\")\n",
    "\n",
    "\n",
    "\n",
    "# --- Prepare Evaluation Dataset (`eval_dataset` for Trainer, `processed_raw_eval_data` for manual loop) ---\n",
    "# (This section remains the same - it loads full dev_data.json, pre-processes fields *for the Dataset object*,\n",
    "#  then maps format_dev_subset to create the 'text' column for eval_dataset)\n",
    "dev_data_path = \"dev_data.json\"\n",
    "print(f\"\\nLoading and preparing evaluation dataset from {dev_data_path}...\")\n",
    "eval_dataset = None # For Trainer\n",
    "processed_raw_eval_data = [] # For manual eval loop\n",
    "try:\n",
    "    with open(dev_data_path, 'r', encoding='utf-8') as f: raw_eval_data = json.load(f)\n",
    "    if isinstance(raw_eval_data, list) and raw_eval_data:\n",
    "        processed_dev_data_for_dataset = []\n",
    "        for record in raw_eval_data:\n",
    "            processed_raw_eval_data.append(record.copy()) # Keep original structure\n",
    "            new_record = {} # Pre-process for Dataset object\n",
    "            for key, value in record.items():\n",
    "                if isinstance(value, dict): new_record[key] = json.dumps(value)\n",
    "                elif isinstance(value, bool): new_record[key] = str(value).lower()\n",
    "                elif isinstance(value, (int, float)): new_record[key] = str(value)\n",
    "                else: new_record[key] = value\n",
    "            processed_dev_data_for_dataset.append(new_record)\n",
    "\n",
    "        all_keys = set().union(*(d.keys() for d in processed_dev_data_for_dataset)); dev_data_dict = {key: [d.get(key) for d in processed_dev_data_for_dataset] for key in all_keys}\n",
    "        dev_dataset_raw = Dataset.from_dict(dev_data_dict)\n",
    "        eval_dataset = dev_dataset_raw.map(format_dev_subset, remove_columns=list(dev_dataset_raw.features), batched=False, desc=\"Formatting eval_dataset\")\n",
    "        original_count = len(eval_dataset); eval_dataset = eval_dataset.filter(lambda x: not x['text'].startswith(\"Error processing\"))\n",
    "        if len(eval_dataset) < original_count: print(f\"Filtered {original_count - len(eval_dataset)} eval records.\")\n",
    "        if len(eval_dataset) == 0: eval_dataset = None\n",
    "        else: print(f\"Formatted `eval_dataset` for Trainer size: {len(eval_dataset)}\")\n",
    "    else: print(f\"Warning: {dev_data_path} empty or not a list.\")\n",
    "except FileNotFoundError: print(f\"Error: {dev_data_path} not found.\")\n",
    "except Exception as e: print(f\"Error loading/processing eval data: {e}\"); print(traceback.format_exc())\n",
    "\n",
    "if eval_dataset is None: print(\"Warning: `eval_dataset` for Trainer is unavailable.\")\n",
    "if not processed_raw_eval_data: print(\"Warning: `processed_raw_eval_data` for manual eval loop is empty.\")\n",
    "\n",
    "print(\"\\n--- Data Preparation Complete ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35ddbec6-758e-4093-b8c4-fba3b11a5854",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7.1: Inspect Formatted Training Data Samples\n",
    "print(\"--- Inspecting Formatted Training Data ---\")\n",
    "\n",
    "# Ensure train_dataset exists from Cell 7\n",
    "if 'train_dataset' in locals() and train_dataset:\n",
    "    num_samples_to_check = 20 # Check a few random samples\n",
    "    indices = random.sample(range(len(train_dataset)), min(num_samples_to_check, len(train_dataset)))\n",
    "\n",
    "    for i in indices:\n",
    "        print(f\"\\n--- Sample Index: {i} ---\")\n",
    "        # It's useful to know the original source, but train_dataset might only have 'text'\n",
    "        # If 'source_key' was kept during Cell 7's final map (it wasn't in the last version):\n",
    "        # source = train_dataset[i].get('source_key', 'Unknown')\n",
    "        # print(f\"Source Key (if available): {source}\")\n",
    "        print(train_dataset[i]['text']) # Print the full formatted text string\n",
    "        print(\"-\" * 60)\n",
    "else:\n",
    "    print(\"`train_dataset` not found or empty. Cannot inspect.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6d67f511-7c9f-4faa-a6e3-be4b57530beb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuring QLoRA...\n",
      "QLoRA configured.\n"
     ]
    }
   ],
   "source": [
    "# Cell 8: QLoRA Configuration\n",
    "from transformers import BitsAndBytesConfig # Ensure imported\n",
    "import torch # Ensure imported\n",
    "\n",
    "print(\"Configuring QLoRA...\")\n",
    "# Use variables defined in Cell 3\n",
    "compute_dtype = getattr(torch, \"bfloat16\" if torch.cuda.is_bf16_supported() else \"float16\")\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=quant_load_in_4bit,\n",
    "    bnb_4bit_quant_type=quant_bnb_4bit_quant_type,\n",
    "    bnb_4bit_compute_dtype=compute_dtype,\n",
    "    bnb_4bit_use_double_quant=quant_bnb_4bit_use_double_quant,\n",
    ")\n",
    "print(\"QLoRA configured.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ee8c57f-5198-435e-af45-67a079ab05e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install bitsandbytes --upgrade --no-cache-dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dee92311-4b3c-41a2-964a-58ef29717b83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading base model with QLoRA config...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1e945a822d14867a3f5c54987393a79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base model loaded.\n"
     ]
    }
   ],
   "source": [
    "# Cell 9: Load Base Model\n",
    "from transformers import AutoModelForCausalLM # Ensure imported\n",
    "from peft import prepare_model_for_kbit_training # Ensure imported\n",
    "import torch # Ensure imported\n",
    "\n",
    "print(\"Loading base model with QLoRA config...\")\n",
    "# Use variables defined in Cell 3\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name, #quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True, #attn_implementation=\"flash_attention_2\", # Keep FA2\n",
    ")\n",
    "model.config.use_cache = False; model.config.pretraining_tp = 1\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "print(\"Base model loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c7e502cf-475d-446c-9c31-2144199f9351",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuring LoRA...\n",
      "LoRA configured.\n"
     ]
    }
   ],
   "source": [
    "# Cell 10: PEFT Configuration (LoRA)\n",
    "from peft import LoraConfig # Ensure imported\n",
    "print(\"Configuring LoRA...\");\n",
    "# Use variables defined in Cell 3\n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha=lora_alpha, lora_dropout=lora_dropout, r=lora_r, bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\", target_modules=lora_target_modules\n",
    ")\n",
    "print(\"LoRA configured.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1dadd37a-8dcd-4f98-ae04-8adea675dae1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying PEFT to the model...\n",
      "trainable params: 97,255,424 || all params: 3,310,005,248 || trainable%: 2.9382\n",
      "PEFT model created.\n"
     ]
    }
   ],
   "source": [
    "# Cell 11: Apply PEFT to the Model\n",
    "from peft import get_peft_model # Ensure imported\n",
    "print(\"Applying PEFT to the model...\"); model = get_peft_model(model, peft_config)\n",
    "model.print_trainable_parameters(); print(\"PEFT model created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb98a4ce-a114-4e12-a885-b692e562af48",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3d4a1cbe-6013-4af5-8b0f-f2956e095f42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defining SFT training configuration (Standard SFT)...\n",
      "Note: Dataset packing is DISABLED.\n",
      "SFT training configuration defined.\n"
     ]
    }
   ],
   "source": [
    "# Cell 12: Training Configuration using SFTConfig (Simplified - No Completion Collator)\n",
    "from trl import SFTConfig # Ensure imported\n",
    "import torch # Ensure imported\n",
    "\n",
    "print(\"Defining SFT training configuration (Standard SFT)...\")\n",
    "\n",
    "# Define evaluation and saving steps\n",
    "eval_steps_value = 500\n",
    "save_steps_value = eval_steps_value\n",
    "\n",
    "# Check if an evaluation dataset was successfully created in Cell 7\n",
    "eval_possible = 'eval_dataset' in locals() and eval_dataset is not None and len(eval_dataset) > 0\n",
    "\n",
    "# Use variables from Cell 3\n",
    "train_batch_size_config = train_batch_size\n",
    "gradient_accumulation_steps_config = gradient_accumulation_steps\n",
    "eval_batch_size_config = eval_batch_size\n",
    "num_train_epochs_config = num_train_epochs # Use epoch setting from Cell 3\n",
    "\n",
    "training_args = SFTConfig(\n",
    "    # --- SFT Specific Args ---\n",
    "    max_length=max_seq_length,          # From Cell 3\n",
    "    # packing=True,                     # Packing can be True or False here, let's try True for efficiency\n",
    "    packing=False,\n",
    "    dataset_text_field=\"text\",          # We are pre-formatting into this column\n",
    "\n",
    "    # --- Standard Training Args ---\n",
    "    output_dir=adapter_output_dir,      # From Cell 3\n",
    "    logging_dir=logs_output_dir,        # From Cell 3\n",
    "    num_train_epochs=num_train_epochs_config, # Use updated epochs\n",
    "    per_device_train_batch_size=train_batch_size_config,\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps_config,\n",
    "    per_device_eval_batch_size=eval_batch_size_config,\n",
    "    eval_strategy=\"steps\" if eval_possible else \"no\",\n",
    "    save_strategy=\"steps\",\n",
    "    eval_steps=eval_steps_value if eval_possible else None,\n",
    "    save_steps=save_steps_value,\n",
    "    load_best_model_at_end=eval_possible,\n",
    "    metric_for_best_model=\"eval_loss\" if eval_possible else None,\n",
    "    greater_is_better=False,\n",
    "    optim=optim_name, # From Cell 3\n",
    "    learning_rate=learning_rate, # From Cell 3\n",
    "    weight_decay=weight_decay, # From Cell 3\n",
    "    max_grad_norm=0.3,\n",
    "    warmup_ratio=warmup_ratio, # From Cell 3\n",
    "    lr_scheduler_type=lr_scheduler_type, # From Cell 3\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=50,\n",
    "    report_to=\"tensorboard\",\n",
    "    save_total_limit=2,\n",
    "    fp16=False,\n",
    "    bf16=torch.cuda.is_bf16_supported(),\n",
    "    seed=seed, data_seed=seed, # From Cell 3\n",
    "    group_by_length=True, # Grouping helps efficiency\n",
    "    gradient_checkpointing=False, # Keep disabled for 80GB GPU\n",
    "    gradient_checkpointing_kwargs={\"use_reentrant\": False},\n",
    ")\n",
    "\n",
    "if not eval_possible: print(\"Warning: Evaluation dataset not available.\")\n",
    "if training_args.packing: print(\"Note: Dataset packing is ENABLED.\")\n",
    "else: print(\"Note: Dataset packing is DISABLED.\")\n",
    "\n",
    "print(\"SFT training configuration defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a862b522-6b56-41e7-97e4-cc5c22a9d013",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a678b3ee-74da-416a-8efe-ec6b1d44e8a0",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing SFTTrainer (Standard Setup)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a56541ebce544ccb93c4c958f9504245",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Converting train dataset to ChatML:   0%|          | 0/88031 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6bf917e753854de08d1dc9c2e809ca9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying chat template to train dataset:   0%|          | 0/88031 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "820906b3b8e84baaad71b2afff7ace0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing train dataset:   0%|          | 0/88031 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1428b6a6e14741d39f9824be7a3c41e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Truncating train dataset:   0%|          | 0/88031 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "171e3530fe154ab88b70147da55d324f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Converting eval dataset to ChatML:   0%|          | 0/800 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b871b4fc68f6417584e588a0967f6410",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying chat template to eval dataset:   0%|          | 0/800 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e894beb80c1840d9ad1e2685fbfca828",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing eval dataset:   0%|          | 0/800 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f4df8e58de4427390011eeba7db3ccc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Truncating eval dataset:   0%|          | 0/800 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SFTTrainer initialized for standard fine-tuning. Starting training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tmishra9/.local/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "The input hidden states seems to be silently casted in float32, this might be related to the fact you have upcasted embedding or layer norm layers in float32. We will cast back the input in torch.bfloat16.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2751' max='2751' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2751/2751 1:08:29, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.598600</td>\n",
       "      <td>1.959437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>1.552400</td>\n",
       "      <td>1.945461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>1.517300</td>\n",
       "      <td>1.925671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>1.505800</td>\n",
       "      <td>1.910042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>1.470400</td>\n",
       "      <td>1.914599</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tmishra9/.local/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/tmishra9/.local/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/tmishra9/.local/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/tmishra9/.local/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/tmishra9/.local/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training completed. Best model saved.\n",
      "***** train metrics *****\n",
      "  eval_samples             =         800\n",
      "  total_flos               = 274969250GF\n",
      "  train_loss               =      1.5459\n",
      "  train_runtime            =  1:08:36.25\n",
      "  train_samples            =       88031\n",
      "  train_samples_per_second =      21.386\n",
      "  train_steps_per_second   =       0.668\n",
      "\n",
      "Saving final model adapters to ./llama3.2-3b-sft-adapters-gen-prompt\n",
      "\n",
      "Training finished and final adapters saved.\n"
     ]
    }
   ],
   "source": [
    "# Cell 13: Initialize Trainer (Standard SFT - CORRECT VARIABLE NAME)\n",
    "import os\n",
    "import torch\n",
    "from trl import SFTTrainer\n",
    "# No need to import DataCollatorForCompletionOnlyLM\n",
    "\n",
    "print(\"Initializing SFTTrainer (Standard Setup)...\")\n",
    "\n",
    "# Ensure necessary variables/objects exist\n",
    "# *** CORRECTED VARIABLE NAME HERE ***\n",
    "if 'train_dataset' not in locals() or train_dataset is None:\n",
    "    raise ValueError(\"`train_dataset` (pre-formatted 'text' column only) not found. Run Cell 7 first.\")\n",
    "# *** END CORRECTION ***\n",
    "if 'tokenizer' not in locals() or tokenizer is None: raise NameError(\"Tokenizer not found.\")\n",
    "if 'training_args' not in locals() or training_args is None: raise NameError(\"Training args (SFTConfig) not found.\")\n",
    "if 'model' not in locals() or model is None: raise NameError(\"Model not found.\")\n",
    "if 'peft_config' not in locals() or peft_config is None: raise NameError(\"PEFT config not found.\")\n",
    "trainer_eval_dataset = eval_dataset if 'eval_dataset' in locals() and eval_dataset is not None else None\n",
    "\n",
    "\n",
    "# --- Initialize SFTTrainer ---\n",
    "# Use the default data collator by not specifying one.\n",
    "# Do NOT pass formatting_func, as data is pre-formatted.\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=train_dataset,       \n",
    "    eval_dataset=trainer_eval_dataset,\n",
    "    peft_config=peft_config,\n",
    "    args=training_args                   \n",
    ")\n",
    "\n",
    "print(\"SFTTrainer initialized for standard fine-tuning. Starting training...\")\n",
    "# --- Run Training ---\n",
    "os.makedirs(training_args.output_dir, exist_ok=True);\n",
    "if training_args.logging_dir: os.makedirs(training_args.logging_dir, exist_ok=True)\n",
    "train_result = trainer.train();\n",
    "# --- Save Metrics / Adapters ---\n",
    "metrics = train_result.metrics; metrics[\"train_samples\"] = len(train_dataset) # Use correct variable\n",
    "if trainer_eval_dataset: metrics[\"eval_samples\"] = len(trainer_eval_dataset); print(f\"\\nTraining completed. Best model saved.\")\n",
    "else: print(\"\\nTraining completed without evaluation.\")\n",
    "trainer.log_metrics(\"train\", metrics); trainer.save_metrics(\"train\", metrics)\n",
    "print(f\"\\nSaving final model adapters to {training_args.output_dir}\")\n",
    "trainer.save_model()\n",
    "print(\"\\nTraining finished and final adapters saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e93bf240-4271-4c14-afba-1dd1aa354f70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Setting up Interactive Chat with FINE-TUNED Model ---\n",
      "Loading base model for fine-tuned chat...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc1fb8052d3b4a0fa7d6c147cc095e53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base model loaded.\n",
      "Loading fine-tuned adapters from: ./llama3.2-3b-sft-adapters-gen-prompt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model 'PeftModelForCausalLM' is not supported for text-generation. Supported models are ['AriaTextForCausalLM', 'BambaForCausalLM', 'BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'LlamaForCausalLM', 'CodeGenForCausalLM', 'CohereForCausalLM', 'Cohere2ForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'DbrxForCausalLM', 'DiffLlamaForCausalLM', 'ElectraForCausalLM', 'Emu3ForCausalLM', 'ErnieForCausalLM', 'FalconForCausalLM', 'FalconMambaForCausalLM', 'FuyuForCausalLM', 'GemmaForCausalLM', 'Gemma2ForCausalLM', 'Gemma3ForConditionalGeneration', 'Gemma3ForCausalLM', 'GitForCausalLM', 'GlmForCausalLM', 'GotOcr2ForConditionalGeneration', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'GraniteForCausalLM', 'GraniteMoeForCausalLM', 'GraniteMoeSharedForCausalLM', 'HeliumForCausalLM', 'JambaForCausalLM', 'JetMoeForCausalLM', 'LlamaForCausalLM', 'MambaForCausalLM', 'Mamba2ForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'MllamaForCausalLM', 'MoshiForCausalLM', 'MptForCausalLM', 'MusicgenForCausalLM', 'MusicgenMelodyForCausalLM', 'MvpForCausalLM', 'NemotronForCausalLM', 'OlmoForCausalLM', 'Olmo2ForCausalLM', 'OlmoeForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'PhimoeForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'RecurrentGemmaForCausalLM', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'WhisperForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM', 'ZambaForCausalLM', 'Zamba2ForCausalLM'].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuned PEFT adapters loaded successfully.\n",
      "Using existing tokenizer.\n",
      "Creating chat generation pipeline...\n",
      "Pipeline created successfully on device: cuda:0\n",
      "\n",
      "--- Starting Chat Session (FINE-TUNED MODEL) ---\n",
      "Enter your instruction/query. Type 'quit' or 'exit' to end.\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  hello\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant thinking... (self-consistency over N samples)\n",
      "Assistant: Hello, what would you like to know?\n",
      "[Time: 5.19s] (from 5 samples)\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  what is 2 + 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant thinking... (self-consistency over N samples)\n",
      "Assistant: 4\n",
      "[Time: 0.62s] (from 5 samples)\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  x-2=5-3x what is x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant thinking... (self-consistency over N samples)\n",
      "Assistant: To solve this equation, we can start by combining like terms on both sides:\n",
      "x-2 = 5-3x\n",
      "x-2 - 3x = 5 - 3x - 2\n",
      "-x = -4\n",
      "To solve for x, we can add 3x to both sides of the equation:\n",
      "-x + 3x = -4 + 3x\n",
      "0 = 3x - 4\n",
      "To solve for x, we can divide both sides of the equation by 3:\n",
      "0 / 3 = (3x - 4) / 3\n",
      "0 = x - 4 / 3\n",
      "To solve for x, we can multiply both sides of the equation by 3:\n",
      "0 * 3 = (3x - 4) * 3\n",
      "0 = 3x - 12\n",
      "To solve for x, we can add 12 to both sides of the equation:\n",
      "0 + 12 = 3x - 12 + 12\n",
      "12 = 3x\n",
      "To solve for x, we can divide both sides of the equation by 3:\n",
      "12 / 3 = 3x / 3\n",
      "4 = x\n",
      "The value of x is 4.\n",
      "#### 4\n",
      "The answer is: 4\n",
      "[Time: 28.85s] (from 5 samples)\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  Magnesium bromide is one of the most abundant compounds found in sea water. Which formula correctly represents magnesium bromide? A. MgBr B. MgBr_{2} C. Mg_{2}Br D. Mg_{3}Br_{2}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant thinking... (self-consistency over N samples)\n",
      "Assistant: B. MgBr_{2}\n",
      "[Time: 1.98s] (from 5 samples)\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  Why do most of the leaves of forest trees grow at the top of the tree? A. to scatter seeds B. to capture sunlight C. to shade the trunk D. to collect water\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant thinking... (self-consistency over N samples)\n",
      "Assistant: A. to scatter seeds\n",
      "[Time: 1.57s] (from 5 samples)\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  Which of the following is the best way to determine whether two people are related? A. Compare their blood types. B. Compare their handwriting. C. Compare their genes. D. Compare their fingerprints.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant thinking... (self-consistency over N samples)\n",
      "Assistant: C. Compare their genes.\n",
      "[Time: 1.56s] (from 5 samples)\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  Can you purchase a dish with injera at Taco Bell? Facts: Taco Bell serves a variety of Mexican and Tex-Mex foods that include tacos, burritos, quesadillas, and nachos. Injera is a sour fermented flatbread with a slightly spongy texture, traditionally made out of teff flour. Injera is part of Ethiopian cuisine.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant thinking... (self-consistency over N samples)\n",
      "Assistant: No, Taco Bell does not serve injera.\n",
      "[Time: 4.45s] (from 5 samples)\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  tell me about nuclear weapons\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant thinking... (self-consistency over N samples)\n",
      "Assistant: A nuclear weapon is a device that can be used to produce a large amount of energy from a small amount of material.  Nuclear weapons can be used to destroy buildings, cities, or even countries.  Nuclear weapons are extremely dangerous and can cause widespread damage if they are used.  Nuclear weapons are often used in war, and are also sometimes used for research and development.  Nuclear weapons are one of the most dangerous weapons in the world, and are not recommended for use in any situation.\n",
      "[Time: 14.51s] (from 5 samples)\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  how do i create a nuclear weapon\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant thinking... (self-consistency over N samples)\n",
      "Assistant: Iâ€™m sorry, Iâ€™m not sure I understand the question.\n",
      "[Time: 4.93s] (from 5 samples)\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  exit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Fine-Tuned Model Chat Session Ended ---\n"
     ]
    }
   ],
   "source": [
    "# Cell 13.5: Interactive Chat with FINE-TUNED Model (Post-Training Check)\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, pipeline, logging\n",
    "from peft import PeftModel # Make sure PeftModel is imported\n",
    "import time\n",
    "import warnings\n",
    "import readline # Optional: Improves input() experience\n",
    "import os # Ensure os is imported if used\n",
    "from collections import Counter\n",
    "\n",
    "print(\"\\n--- Setting up Interactive Chat with FINE-TUNED Model ---\")\n",
    "\n",
    "# --- Configuration (Ensure these are available from previous cells) ---\n",
    "# model_name = \"meta-llama/Llama-3.2-3B\"\n",
    "# adapter_output_dir = \"./llama3.2-3b-sft-adapters-gen-prompt\" # Make sure this matches Cell 3/12\n",
    "# bnb_config = ... # Make sure bnb_config from Cell 8 is available\n",
    "\n",
    "# --- Load Fine-Tuned Model and Tokenizer ---\n",
    "chat_model_ft = None\n",
    "chat_tokenizer_ft = None\n",
    "chat_pipe_ft = None\n",
    "model_loaded_ok = False\n",
    "\n",
    "try:\n",
    "    # Check if essential config exists\n",
    "    if 'model_name' not in locals() or 'bnb_config' not in locals() or 'adapter_output_dir' not in locals():\n",
    "        raise NameError(\"Essential config (model_name, bnb_config, adapter_output_dir) not found.\")\n",
    "\n",
    "    print(\"Loading base model for fine-tuned chat...\")\n",
    "    base_model_chat = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        #quantization_config=bnb_config, # Use the same quantization\n",
    "        device_map=\"auto\",\n",
    "        trust_remote_code=True,\n",
    "        #attn_implementation=\"flash_attention_2\", # Use FA2 if trained with it\n",
    "    )\n",
    "    print(\"Base model loaded.\")\n",
    "\n",
    "    print(f\"Loading fine-tuned adapters from: {adapter_output_dir}\")\n",
    "    # Make sure the path exists\n",
    "    if not os.path.isdir(adapter_output_dir):\n",
    "         raise FileNotFoundError(f\"Adapter directory not found: {adapter_output_dir}. Did training save correctly?\")\n",
    "\n",
    "    chat_model_ft = PeftModel.from_pretrained(base_model_chat, adapter_output_dir)\n",
    "    # chat_model_ft = chat_model_ft.merge_and_unload() # Optional: Merge for potential speedup, uses more VRAM\n",
    "    chat_model_ft.eval() # Set to evaluation mode\n",
    "    print(\"Fine-tuned PEFT adapters loaded successfully.\")\n",
    "\n",
    "    # Load tokenizer (use the same one as training/eval)\n",
    "    if 'tokenizer' in locals() and tokenizer is not None:\n",
    "         chat_tokenizer_ft = tokenizer\n",
    "         chat_tokenizer_ft.padding_side = \"left\" # Set for generation\n",
    "         print(\"Using existing tokenizer.\")\n",
    "    else:\n",
    "         print(\"Reloading tokenizer for chat...\")\n",
    "         chat_tokenizer_ft = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True, use_fast=False, legacy=False)\n",
    "         chat_tokenizer_ft.padding_side = \"left\"\n",
    "         if chat_tokenizer_ft.pad_token is None: chat_tokenizer_ft.pad_token = chat_tokenizer_ft.eos_token\n",
    "         print(\"Tokenizer loaded.\")\n",
    "\n",
    "    model_loaded_ok = True\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"ERROR: Failed to load fine-tuned model/tokenizer for chat: {e}\")\n",
    "    import traceback\n",
    "    print(traceback.format_exc())\n",
    "\n",
    "# --- Create Chat Pipeline ---\n",
    "if model_loaded_ok:\n",
    "    print(\"Creating chat generation pipeline...\")\n",
    "    logging.set_verbosity(logging.ERROR)\n",
    "    warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "    try:\n",
    "        chat_pipe_ft = pipeline(\n",
    "            \"text-generation\",\n",
    "            model=chat_model_ft,\n",
    "            tokenizer=chat_tokenizer_ft,\n",
    "            pad_token_id=chat_tokenizer_ft.eos_token_id\n",
    "        )\n",
    "        print(f\"Pipeline created successfully on device: {chat_model_ft.device}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating pipeline: {e}\")\n",
    "        model_loaded_ok = False # Can't chat if pipeline fails\n",
    "    finally:\n",
    "        logging.set_verbosity(logging.WARNING)\n",
    "        warnings.filterwarnings(\"default\", category=UserWarning)\n",
    "\n",
    "\n",
    "# --- Interactive Chat Loop ---\n",
    "if model_loaded_ok and chat_pipe_ft:\n",
    "    print(\"\\n--- Starting Chat Session (FINE-TUNED MODEL) ---\")\n",
    "    print(\"Enter your instruction/query. Type 'quit' or 'exit' to end.\")\n",
    "    print(\"-\" * 30)\n",
    "\n",
    "    # Ensure create_prompt function is available (should be defined in Cell 6)\n",
    "    if 'create_prompt' not in locals():\n",
    "        print(\"ERROR: create_prompt function not found. Cannot format prompts.\")\n",
    "    else:\n",
    "        while True:\n",
    "            try:\n",
    "                user_input = input(\"You: \")\n",
    "            except EOFError:\n",
    "                print(\"\\nEOF detected, ending chat.\")\n",
    "                break\n",
    "            if user_input.lower().strip() in [\"quit\", \"exit\"]:\n",
    "                break\n",
    "            if not user_input.strip():\n",
    "                continue\n",
    "\n",
    "            # Format using the SAME prompt structure used for training (Zero-Shot for inference)\n",
    "            # Treat the entire user input as the instruction for a general assistant\n",
    "            chat_instruction = user_input\n",
    "            # No separate input field unless you design logic to parse it\n",
    "            chat_input = None\n",
    "            # Create the prompt, ensuring no answer/EOS token is included\n",
    "            chat_prompt = create_prompt(chat_instruction, chat_input, output=\"\", include_eos=False)\n",
    "\n",
    "            # Remove the trailing '### Response:' marker before sending to pipeline\n",
    "            response_marker = \"\\n### Response:\"\n",
    "            if chat_prompt.endswith(response_marker):\n",
    "                chat_prompt = chat_prompt[:-len(response_marker)].rstrip()\n",
    "\n",
    "            print(f\"Assistant thinking... (self-consistency over N samples)\")\n",
    "            # Generate response with sampling\n",
    "            try:\n",
    "                start_time = time.time()\n",
    "                \n",
    "                samples = 5\n",
    "                output_list = []\n",
    "\n",
    "                for _ in range(samples):\n",
    "                    \n",
    "                    outputs = chat_pipe_ft(\n",
    "                        chat_prompt,\n",
    "                        max_new_tokens=512,       # Max length for the response part\n",
    "                        do_sample=True,\n",
    "                        temperature=0.7,\n",
    "                        top_p=0.9,\n",
    "                        return_full_text=False   # Only get the generated text\n",
    "                    )\n",
    "                    #end_time = time.time()\n",
    "\n",
    "                    if outputs and isinstance(outputs, list) and 'generated_text' in outputs[0]:\n",
    "                        response_text = outputs[0]['generated_text'].strip()\n",
    "                        # Clean up potential EOS token\n",
    "                        if response_text.endswith(chat_tokenizer_ft.eos_token):\n",
    "                            response_text = response_text[:-len(chat_tokenizer_ft.eos_token)].strip()\n",
    "                        output_list.append(response_text)\n",
    "\n",
    "                end_time = time.time()\n",
    "\n",
    "                if output_list:\n",
    "                    most_common_response, count = Counter(output_list).most_common(1)[0]\n",
    "                    print(f\"Assistant: {most_common_response}\")\n",
    "                    print(f\"[Time: {end_time - start_time:.2f}s] (from {samples} samples)\")\n",
    "                else:\n",
    "                    print(\"Assistant: [Error - No responses generated or unexpected format]\")\n",
    "\n",
    "\n",
    "                    #print(f\"Assistant: {response_text}\")\n",
    "                    #print(f\"Raw output: {outputs}\") # Debug raw output\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Assistant: [Error generating response: {e}]\")\n",
    "                print(traceback.format_exc())\n",
    "\n",
    "            print(\"-\" * 30)\n",
    "\n",
    "        print(\"\\n--- Fine-Tuned Model Chat Session Ended ---\")\n",
    "else:\n",
    "    print(\"\\nChat cannot start because the fine-tuned model or pipeline failed to load.\")\n",
    "\n",
    "# Optional: Clean up memory\n",
    "# del chat_pipe_ft\n",
    "# del chat_model_ft\n",
    "# if 'base_model_chat' in locals(): del base_model_chat # Careful if reusing base\n",
    "# torch.cuda.empty_cache()\n",
    "# print(\"Chat resources released (attempted).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32e95c61-9a22-4b28-9b51-e85617238199",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4c47fd1-b283-4d5e-a9c4-91be83dd6794",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d102e4f5-cf12-4b76-bba7-6b991e7904d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eb8faf0-9ba6-4377-a970-7f6d8a21d03b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40eeb545-a74d-4c81-a1d2-aa7024c6994f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
